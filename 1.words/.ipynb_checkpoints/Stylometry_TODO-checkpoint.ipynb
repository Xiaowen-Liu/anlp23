{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73423b4a",
   "metadata": {},
   "source": [
    "Many fundamental features of authorial style can be implemented as simple functions over counts of words and sentences.  In this homework, you'll be implementing these functions to measure stylistic properties of a text.\n",
    "\n",
    "For a detailed history of many of these techniques, see Grieve 2005, \"[Quantiative Authorship Attribution: A History and Evaluation of Techniques](https://summit.sfu.ca/_flysystem/fedora/sfu_migrate/8840/etd1721.pdf)\" (note this was a master's thesis).\n",
    "\n",
    "Before working on this notebook, install seaborn:\n",
    "\n",
    "```conda install seaborn```\n",
    "\n",
    "This homework has 3 required questions (Q1-3) and one optional \"check-plus\" question (Q4) at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef7371",
   "metadata": {},
   "source": [
    "First, let's read in some works of fiction by three different authors: [Henry James](https://en.wikipedia.org/wiki/Henry_James), [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen) and [Horatio Alger](https://en.wikipedia.org/wiki/Horatio_Alger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a82528",
   "metadata": {},
   "outputs": [],
   "source": [
    "james=[]\n",
    "for filename in [\"../data/stylometry/208_daisy_miller_a_study.txt\", \"../data/stylometry/432_the_ambassadors.txt\", \"../data/stylometry/209_the_turn_of_the_screw.txt\"]:\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        james.append(file.read())\n",
    "\n",
    "austen=[]\n",
    "for filename in [\"../data/stylometry/158_emma.txt\", \"../data/stylometry/105_persuasion.txt\", \"../data/stylometry/1342_pride_and_prejudice.txt\"]:\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        austen.append(file.read())\n",
    "\n",
    "        \n",
    "alger=[]\n",
    "for filename in [\"../data/stylometry/18581_adrift_in_new_york_tom_and_florence_braving_the_world.txt\", \"../data/stylometry/5348_ragged_dick_or_street_life_in_new_york_with_the_bootblacks.txt\", \"../data/stylometry/21632_fame.txt\"]:\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        alger.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de53c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lib_laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e98e4",
   "metadata": {},
   "source": [
    "**Q1.** First, implement the average sentence length.  Write a function that takes a single text as input and returns the average length of sentences within it using `nltk.tokenize.word_tokenize` and `nltk.tokenize.sent_tokenize`.  The output should be a single real number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b2ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sentence_length(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text) # break into sentences\n",
    "#     print(sentences)\n",
    "    sentence_length = []\n",
    "    for i in sentences:\n",
    "        tokens = nltk.tokenize.word_tokenize(i)\n",
    "#         print(tokens)\n",
    "        sentence_length.append(len(tokens))\n",
    "    return np.average(sentence_length)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afba8a",
   "metadata": {},
   "source": [
    "Test that function on texts by James, Austen and Alger (just execute this cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa788b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James avg sentences: 23.482\n",
      "James avg sentences: 21.118\n",
      "James avg sentences: 30.821\n",
      "Austen avg sentences: 32.223\n",
      "Austen avg sentences: 26.785\n",
      "Austen avg sentences: 30.496\n",
      "Alger avg sentences: 13.628\n",
      "Alger avg sentences: 15.989\n",
      "Alger avg sentences: 14.881\n"
     ]
    }
   ],
   "source": [
    "for book in james:\n",
    "    avg_sents=get_avg_sentence_length(book)\n",
    "    print(\"James avg sentences: %.3f\" % avg_sents)\n",
    "\n",
    "for book in austen:\n",
    "    avg_sents=get_avg_sentence_length(book)\n",
    "    print(\"Austen avg sentences: %.3f\" % avg_sents)\n",
    "\n",
    "for book in alger:\n",
    "    avg_sents=get_avg_sentence_length(book)\n",
    "    print(\"Alger avg sentences: %.3f\" % avg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c111411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_sentence_length(\"OMG, Hi! I'm Xiaowen, nice to meet you.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6821eff",
   "metadata": {},
   "source": [
    "**Q2**: Use the `nltk.word_tokenize` method to implement the type-token ratio (a measure of vocabulary diversity):\n",
    "\n",
    "$$\n",
    "\\textrm{TTR} = {\\textrm{number of distinct word types} \\over \\textrm{number of word tokens}}\n",
    "$$\n",
    "\n",
    "\n",
    "TTR is dependent on text length (intuitively, the longer a text is, the greater chance you have of a word type repeating), so this number is only comparable between documents of identical lengths.  Calculate this measure for the first 500 words of you document and report the results here. Exclude tokens that are exclusively punctuation from all counts, and calculate this measure over the lowercased version of that text.  (You can find puncutation with `string.punctuation`.)  Your `type_token_ratio` function should take a single input as argument (a text at least 500 tokens long) and output a single real number (the TTR for that text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "051358e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def type_token_ratio(text, num_words=500):\n",
    "    # extract the first 500 words\n",
    "    # break into tokens and remove punctuation\n",
    "    # put the text into lowercase and calculate \n",
    "    extracted_text = text[:501]\n",
    "    extracted_text_no_punc_lower = extracted_text.translate(string.punctuation).lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(extracted_text_no_punc_lower)\n",
    "    return len(Counter(tokens).keys())/len(tokens)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531fc14",
   "metadata": {},
   "source": [
    "Again, test that function on texts by James, Austen and Alger (just execute this cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147b208d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James avg: 0.621\n",
      "James avg: 0.759\n",
      "James avg: 0.630\n",
      "Austen avg: 0.703\n",
      "Austen avg: 0.776\n",
      "Austen avg: 0.717\n",
      "Alger avg: 0.520\n",
      "Alger avg: 0.661\n",
      "Alger avg: 0.784\n"
     ]
    }
   ],
   "source": [
    "for book in james:\n",
    "    ttr=type_token_ratio(book)\n",
    "    print(\"James avg: %.3f\" % ttr)\n",
    "\n",
    "for book in austen:\n",
    "    ttr=type_token_ratio(book)\n",
    "    print(\"Austen avg: %.3f\" % ttr)\n",
    "\n",
    "for book in alger:\n",
    "    ttr=type_token_ratio(book)\n",
    "    print(\"Alger avg: %.3f\" % ttr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c24f1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_ratio(\"to be or not to be\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ace5a",
   "metadata": {},
   "source": [
    "**Q3.** Finally, count up the token frequency of function words (these are determiners like \"the\", prepositions like \"in\" and conjunctions like \"and\" and \"because\").  Let's approximate \"function words\" by counting the token frequency of `nltk.corpus.stopwords`.  Here's that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "958fe012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lib_laptop\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 {'that', 'over', 'me', 'are', 'into', 'she', 'm', 'there', 'just', 'can', 'once', 'him', 'have', 'own', 'at', 'we', 'your', 'with', 'yourselves', 'y', \"hadn't\", 'don', 'wouldn', \"needn't\", 'if', 'no', 'each', 'few', \"you'd\", 'of', 'before', 'under', 'from', 'will', 'too', 'their', \"should've\", \"won't\", \"that'll\", 'these', 'to', 'is', \"aren't\", 'because', 'isn', 'about', 'here', \"she's\", 'do', 'why', 'further', 'nor', \"it's\", 'i', 'doing', 'couldn', 'does', \"you'll\", 'most', 'hers', 'his', 'won', 'then', 'll', 've', 'haven', 'whom', 'ourselves', 'below', 'as', 'which', 'than', 'had', 'between', 'itself', \"mightn't\", 'when', 'ma', 'be', \"couldn't\", 'who', 'where', 'yours', 'they', 'aren', 'was', 'it', \"didn't\", 'both', \"isn't\", \"shan't\", 'has', 'or', 'other', 'were', 'a', 't', 'being', 'for', 'after', 'any', 'those', 'up', 'd', 'only', 'what', 'them', 'during', 'off', 'down', 'above', 's', 'all', 'by', 'until', 'didn', 'himself', 're', \"weren't\", 'not', 'in', 'its', 'myself', 'my', 'mightn', 'did', 'how', 'themselves', \"hasn't\", 'out', \"you've\", 'theirs', 'herself', 'am', 'ain', \"you're\", \"wouldn't\", 'while', 'now', \"doesn't\", 'weren', 'so', 'hasn', 'this', \"haven't\", 'again', 'o', 'very', \"wasn't\", 'some', 'her', 'been', 'wasn', 'needn', \"mustn't\", 'an', 'yourself', 'shan', 'through', 'having', 'the', 'but', 'hadn', 'ours', 'he', 'you', \"don't\", 'against', 'shouldn', 'same', 'mustn', 'our', 'doesn', \"shouldn't\", 'and', 'such', 'on', 'more', 'should'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "print(len(nltk_stopwords), nltk_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a9d27",
   "metadata": {},
   "source": [
    "Write a function `count_stopword_freqs` that takes a single text as input and returns a dictionary containing frequency of each of these terms within it -- e.g.:\n",
    "\n",
    "{\"wouldn\": 0.00003, ..., \"are\": 0.0004}\n",
    "\n",
    "Frequency is simply the count of that term in the text divided by the count of all tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e67118ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords_freqs(book):\n",
    "    # tokenize the book\n",
    "    # go over each element in the book, if it's in stopwords, add to the dictionary\n",
    "    lower_book = book.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(lower_book)\n",
    "    stopwords_freq = dict.fromkeys(nltk_stopwords, 0)\n",
    "    for i in tokens:\n",
    "        if i in nltk_stopwords:\n",
    "            stopwords_freq[i] += (1/len(tokens))\n",
    "    return stopwords_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ba2a0",
   "metadata": {},
   "source": [
    "Again, test that function on texts by James, Austen and Alger (just execute this cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5f458b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James 'because': 0.00026\n",
      "James 'because': 0.00048\n",
      "James 'because': 0.00025\n",
      "Austen 'because': 0.00028\n",
      "Austen 'because': 0.00027\n",
      "Austen 'because': 0.00044\n",
      "Alger 'because': 0.00037\n",
      "Alger 'because': 0.00034\n",
      "Alger 'because': 0.00052\n"
     ]
    }
   ],
   "source": [
    "for book in james:\n",
    "    freqs=count_stopwords_freqs(book)\n",
    "    print(\"James 'because': %.5f\" % freqs[\"because\"])\n",
    "\n",
    "for book in austen:\n",
    "    freqs=count_stopwords_freqs(book)\n",
    "    print(\"Austen 'because': %.5f\" % freqs[\"because\"])\n",
    "\n",
    "for book in alger:\n",
    "    freqs=count_stopwords_freqs(book)\n",
    "    print(\"Alger 'because': %.5f\" % freqs[\"because\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcfbf35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 0,\n",
       " 'over': 0,\n",
       " 'me': 0.14285714285714285,\n",
       " 'are': 0,\n",
       " 'into': 0,\n",
       " 'she': 0,\n",
       " 'm': 0,\n",
       " 'there': 0,\n",
       " 'just': 0,\n",
       " 'can': 0,\n",
       " 'once': 0,\n",
       " 'him': 0,\n",
       " 'have': 0,\n",
       " 'own': 0,\n",
       " 'at': 0,\n",
       " 'we': 0,\n",
       " 'your': 0,\n",
       " 'with': 0,\n",
       " 'yourselves': 0,\n",
       " 'y': 0,\n",
       " \"hadn't\": 0,\n",
       " 'don': 0,\n",
       " 'wouldn': 0,\n",
       " \"needn't\": 0,\n",
       " 'if': 0,\n",
       " 'no': 0,\n",
       " 'each': 0,\n",
       " 'few': 0,\n",
       " \"you'd\": 0,\n",
       " 'of': 0,\n",
       " 'before': 0,\n",
       " 'under': 0,\n",
       " 'from': 0,\n",
       " 'will': 0,\n",
       " 'too': 0,\n",
       " 'their': 0,\n",
       " \"should've\": 0,\n",
       " \"won't\": 0,\n",
       " \"that'll\": 0,\n",
       " 'these': 0,\n",
       " 'to': 0.14285714285714285,\n",
       " 'is': 0,\n",
       " \"aren't\": 0,\n",
       " 'because': 0,\n",
       " 'isn': 0,\n",
       " 'about': 0,\n",
       " 'here': 0,\n",
       " \"she's\": 0,\n",
       " 'do': 0,\n",
       " 'why': 0,\n",
       " 'further': 0,\n",
       " 'nor': 0,\n",
       " \"it's\": 0,\n",
       " 'i': 0,\n",
       " 'doing': 0,\n",
       " 'couldn': 0,\n",
       " 'does': 0,\n",
       " \"you'll\": 0,\n",
       " 'most': 0,\n",
       " 'hers': 0,\n",
       " 'his': 0,\n",
       " 'won': 0,\n",
       " 'then': 0,\n",
       " 'll': 0,\n",
       " 've': 0,\n",
       " 'haven': 0,\n",
       " 'whom': 0,\n",
       " 'ourselves': 0,\n",
       " 'below': 0,\n",
       " 'as': 0,\n",
       " 'which': 0,\n",
       " 'than': 0,\n",
       " 'had': 0,\n",
       " 'between': 0,\n",
       " 'itself': 0,\n",
       " \"mightn't\": 0,\n",
       " 'when': 0,\n",
       " 'ma': 0,\n",
       " 'be': 0,\n",
       " \"couldn't\": 0,\n",
       " 'who': 0,\n",
       " 'where': 0,\n",
       " 'yours': 0,\n",
       " 'they': 0,\n",
       " 'aren': 0,\n",
       " 'was': 0,\n",
       " 'it': 0,\n",
       " \"didn't\": 0,\n",
       " 'both': 0,\n",
       " \"isn't\": 0,\n",
       " \"shan't\": 0,\n",
       " 'has': 0,\n",
       " 'or': 0,\n",
       " 'other': 0,\n",
       " 'were': 0,\n",
       " 'a': 0,\n",
       " 't': 0,\n",
       " 'being': 0,\n",
       " 'for': 0,\n",
       " 'after': 0,\n",
       " 'any': 0,\n",
       " 'those': 0,\n",
       " 'up': 0,\n",
       " 'd': 0,\n",
       " 'only': 0,\n",
       " 'what': 0,\n",
       " 'them': 0,\n",
       " 'during': 0,\n",
       " 'off': 0,\n",
       " 'down': 0,\n",
       " 'above': 0,\n",
       " 's': 0,\n",
       " 'all': 0,\n",
       " 'by': 0,\n",
       " 'until': 0,\n",
       " 'didn': 0,\n",
       " 'himself': 0,\n",
       " 're': 0,\n",
       " \"weren't\": 0,\n",
       " 'not': 0,\n",
       " 'in': 0,\n",
       " 'its': 0,\n",
       " 'myself': 0,\n",
       " 'my': 0,\n",
       " 'mightn': 0,\n",
       " 'did': 0,\n",
       " 'how': 0,\n",
       " 'themselves': 0,\n",
       " \"hasn't\": 0,\n",
       " 'out': 0,\n",
       " \"you've\": 0,\n",
       " 'theirs': 0,\n",
       " 'herself': 0,\n",
       " 'am': 0,\n",
       " 'ain': 0,\n",
       " \"you're\": 0,\n",
       " \"wouldn't\": 0,\n",
       " 'while': 0,\n",
       " 'now': 0,\n",
       " \"doesn't\": 0,\n",
       " 'weren': 0,\n",
       " 'so': 0,\n",
       " 'hasn': 0,\n",
       " 'this': 0,\n",
       " \"haven't\": 0,\n",
       " 'again': 0,\n",
       " 'o': 0,\n",
       " 'very': 0,\n",
       " \"wasn't\": 0,\n",
       " 'some': 0,\n",
       " 'her': 0,\n",
       " 'been': 0,\n",
       " 'wasn': 0,\n",
       " 'needn': 0,\n",
       " \"mustn't\": 0,\n",
       " 'an': 0,\n",
       " 'yourself': 0,\n",
       " 'shan': 0,\n",
       " 'through': 0,\n",
       " 'having': 0,\n",
       " 'the': 0.14285714285714285,\n",
       " 'but': 0,\n",
       " 'hadn': 0,\n",
       " 'ours': 0,\n",
       " 'he': 0,\n",
       " 'you': 0,\n",
       " \"don't\": 0,\n",
       " 'against': 0,\n",
       " 'shouldn': 0,\n",
       " 'same': 0,\n",
       " 'mustn': 0,\n",
       " 'our': 0,\n",
       " 'doesn': 0,\n",
       " \"shouldn't\": 0,\n",
       " 'and': 0,\n",
       " 'such': 0,\n",
       " 'on': 0,\n",
       " 'more': 0,\n",
       " 'should': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords_freqs(\"The nightmare wouldn't come to me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae23125",
   "metadata": {},
   "source": [
    "The functions you wrote now give you a way represent a text as a vector of stylistic features.  Let's see how we can use that to compare the similarity of styles between the three authors above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7989f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_style(text):\n",
    "    vec=[]\n",
    "    vec.append(get_avg_sentence_length(text))\n",
    "    vec.append(type_token_ratio(text))\n",
    "    stopword_freq=count_stopwords_freqs(text)\n",
    "    for stop in set(stopwords.words('english')):\n",
    "        if stop in stopword_freq:\n",
    "            vec.append(stopword_freq[stop])\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3737940",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vecs=[]\n",
    "for book in james:\n",
    "    all_vecs.append(featurize_style(book))\n",
    "\n",
    "for book in austen:\n",
    "    all_vecs.append(featurize_style(book))\n",
    "\n",
    "for book in alger:\n",
    "    all_vecs.append(featurize_style(book))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd59b4",
   "metadata": {},
   "source": [
    "We're going to compare the style vector for each book with each other, comparing them via cosine similarity. To give equal weight to each style dimension, we'll normalize each feature to its z score (with respect to the value of that feature across all observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706876a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_vecs)\n",
    "all_vecs=scaler.transform(all_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims=cosine_similarity(all_vecs, all_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db410987",
   "metadata": {},
   "source": [
    "Now let's visualize those similarites as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78294f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac222da",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"james1\", \"james2\", \"james3\", \"austen1\", \"austen2\", \"austen3\", \"alger1\", \"alger2\", \"alger3\"]\n",
    "df = pd.DataFrame(sims, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = sns.heatmap(df, cmap=\"Greens\", xticklabels=names, yticklabels=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20802c54",
   "metadata": {},
   "source": [
    "Q4 **(check-plus)**: From the Grieve 2005, \"[Quantiative Authorship Attribution: A History and Evaluation of Techniques](https://summit.sfu.ca/_flysystem/fedora/sfu_migrate/8840/etd1721.pdf)\" paper referenced above, pick one other stylistic feature and implement it.  As with the other stylistic measures, run it on the 9 books above; does it differentiate those authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23668e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
